platforms/gym/code/modification_on_2d_env.py
29:# TOPIC: (DRL) Modification on a "gym 2d env" For DRL Training
76:# TOPIC: (GYM) How to Convert an Ordinary Environment's States to State-Buffers for Motion Sensing
platforms/tensorflow/keras/01_custom_network_implementation.py
31:# TOPIC: (GEN) Ensambeled Network Design in tf2
samples/DRL/DQN/keras_ddqn_agent.py
98:    # TOPIC: (DRL/DQN) DDQN Learning Algorithm - keras
samples/DRL/DQN/keras_ddqn_cnn_agent.py
19:        # TOPIC: (DRL/GEN) State-Memory for Environment with Motion
70:# TOPIC: (DRL/DQN) CNN Network for agent with input states as images
76:    # TOPIC: (DRL/DQN) The Structure of a Convolutional Layer in DQN
79:    # TOPIC: (DRL/DQN) Training Agent with Time-Varying Environment and CNN Brain
124:        # TOPIC: Input 2D (Image) Observarion to a DRL Agent in Environment with Motion
samples/DRL/DQN/keras_ddqn_cnn_training.py
45:    # TOPIC: (DRL/DQN) DQN Training Algorithm
samples/DRL/DQN/keras_dqn_agent.py
30:        # TOPIC: (DRL/GEN) When is One-Hot Encoding Necessary?
42:            # TOPIC: (DRL/GEN) Performing One-Hot Encoding on the Network Outputs
132:            # TOPIC: Reverse of One-Hot Encoding
samples/DRL/DQN/tf2_dqn_agent.py
69:    # TOPIC: (DRL/GEN) The Experience Replay
97:# TOPIC: (DRL/DQN) Build the Deep Q-Network in tf2
108:# TOPIC: (DRL/DQN) A Simple DQN Agent - tf2
150:    # TOPIC: (DRL) Epsilon-Greedy Action Selection
161:    # TOPIC: (DRL) Practical Difference Between Value-Based and Policy-Based DRL
176:        # TOPIC: (DRL/DQN) Simple DQN Learning Algorithm
samples/DRL/DQN/tf2_dueling_ddqn_agent.py
14:# TOPIC: (DRL/DQN) Dueling DQN Network Architecture
120:# TOPIC: (DRL/DQN) A Dueling DQN Agent 
166:    # TOPIC: (DRL/DQN) DDQN Learning Algorithm - tf2
samples/DRL/reinforce/keras_policy_gradient_agent.py
67:        # TOPIC: (DRL/GEN) A Custom Loss Function in Keras (no tf)
93:        # TOPIC: (DRL/PG) Defining a mirror network
146:        # TOPIC: (DRL/GEN) Performing One-Hot Encoding on the Network Outputs
166:        # TOPIC: (DRL/GEN) Zero-Based Normalization on Network Inputs
176:        # TOPIC: (DRL/PG) Network Inputs in DRL
samples/DRL/reinforce/tf2_policy_gradient_agent.py
42:        # TOPIC: (GEN) The Input to tf2's Dense Layers
48:        # TOPIC: (DRL/PG) Using a Network in a Single Time-Step (forward pass):
54:        # TOPIC: (DRL/PG) Probabilistic Action Selection 
61:        # TOPIC: (DRL/PG) Passing Network Output (action) to OpenAI Gym
89:        # TOPIC: (DRL/PG) How to Calculate Discounted Sum of Future Rewards (DSoFR) from Now On (G_t or R_t)
samples/DRL/reinforce/tf2_policy_gradient_training.py
19:    # TOPIC: (DRL) Training a DRL Network Based on the Monte-Carlo Method
samples/MLP/tf_02_first_nn_implementation.py
33:# TOPIC: (GEN) Dataset Preparation - tf
57:# TOPIC: (MLP) Model Design in tf2 - 1
97:# TOPIC: (GEN) Model Compile in tf2
140:# TOPIC: (MLP) Model Training in tf2
158:# TOPIC: (MLP) Model Evaluation in tf2
164:# TOPIC: (MLP) Using the Model in tf2
topics.txt
2:29:# TOPIC: (DRL) Modification on a "gym 2d env" For DRL Training
3:76:# TOPIC: (GYM) How to Convert an Ordinary Environment's States to State-Buffers for Motion Sensing
5:31:# TOPIC: (GEN) Ensambeled Network Design in tf2
7:98:    # TOPIC: (DRL/DQN) DDQN Learning Algorithm - keras
9:19:        # TOPIC: (DRL/GEN) State-Memory for Environment with Motion
10:70:# TOPIC: (DRL/DQN) CNN Network for agent with input states as images
11:76:    # TOPIC: (DRL/DQN) The Structure of a Convolutional Layer in DQN
12:79:    # TOPIC: (DRL/DQN) Training Agent with Time-Varying Environment and CNN Brain
13:124:        # TOPIC: Input 2D (Image) Observarion to a DRL Agent in Environment with Motion
15:45:    # TOPIC: (DRL/DQN) DQN Training Algorithm
17:30:        # TOPIC: (DRL/GEN) When is One-Hot Encoding Necessary?
18:42:            # TOPIC: (DRL/GEN) Performing One-Hot Encoding on the Network Outputs
19:132:            # TOPIC: Reverse of One-Hot Encoding
21:69:    # TOPIC: (DRL/GEN) The Experience Replay
22:97:# TOPIC: (DRL/DQN) Build the Deep Q-Network in tf2
23:108:# TOPIC: (DRL/DQN) A Simple DQN Agent - tf2
24:150:    # TOPIC: (DRL) Epsilon-Greedy Action Selection
25:161:    # TOPIC: (DRL) Practical Difference Between Value-Based and Policy-Based DRL
26:176:        # TOPIC: (DRL/DQN) Simple DQN Learning Algorithm
28:14:# TOPIC: (DRL/DQN) Dueling DQN Network Architecture
29:120:# TOPIC: (DRL/DQN) A Dueling DQN Agent 
30:166:    # TOPIC: (DRL/DQN) DDQN Learning Algorithm - tf2
32:67:        # TOPIC: (DRL/GEN) A Custom Loss Function in Keras (no tf)
33:93:        # TOPIC: (DRL/PG) Defining a mirror network
34:146:        # TOPIC: (DRL/GEN) Performing One-Hot Encoding on the Network Outputs
35:166:        # TOPIC: (DRL/GEN) Zero-Based Normalization on Network Inputs
36:176:        # TOPIC: (DRL/PG) Network Inputs in DRL
